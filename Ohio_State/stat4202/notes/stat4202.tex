% !TEX TS-program = xelatex
\documentclass[11pt]{article}
\usepackage{lindrew}
\usepackage{xcolor}
\usepackage{fontspec}

\title{Stat 4202: Mathematical Statistics II}
\author{Lecturer: \textbf{Professor Andrew Kerr}\\Notes by: Farhan Sadeek}
\date{Spring 2025}

\begin{document}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{January 6, 2025}
STAT 4202 will rely a lot on STAT 4201. So we need to have a pretty good
understanding of those concepts.

\subsection{Review of Probability Theory}
\begin{definition}
    The \vocab{Sample Space}, denoted by $\mathcal{S}$, is the set of all outcomes from an experiment.
\end{definition}

\begin{definition}
    An \vocab{Event}, usually denoted by a capital letter such as $A$ or $B$, is a subset of the Sample Space.
\end{definition}

The probability function
\begin{itemize}
    \item $P(A) \geq 0$
    \item $P(\mathcal{S}) = 1$
    \item For disjoint sets $A_1$, $A_2$, $\cdots$, $A_n$: \[ P\left(\bigcup_{i = 1}^{n}A_i \right) = \sum_{i = 1}^{n} P(A_i)\]
\end{itemize}

If an event $A$ is a subset of another event $B$, then the probability of $A$
is less than or equal to the probability of event $B$. That is to say, if $A
    \subseteq B$, then \( P(A) \leq P(B)\)

The complement of an event $A$, denoted by $A^c$, has a probability equal to
one minus the probability of the event $A$. That is,
\[ P(A^c) = 1 - P(A) \]

A partition of a sample space $\mathcal{S}$ is an exhaustive, non-overlapping
collection of events $A_1$, $A_2$, $\cdots$, $A_n$ that is exhaustive and
mutually exclusive: \[\bigcup_{i=1}^{n} A_i = \mathcal{S}\] and \[A_i \cap A_j = \emptyset \quad \forall i \neq j\]

For any partition, we have \[\sum_{i=1}^{n} P(A_i) = 1\]

Two events $A$ and $B$ are \vocab{independent} if the outcome of one doesn't
affect the likelihood of the occurrence of the other. For two independent
events, we have \[P(A \cap B) = P(A)P(B)\]

The \vocab{conditional probability} of $A$ given $B$ is given by \[P(A|B) = \frac{P(A \cap B)}{P(B)}\]

\begin{lemma}
    Note that if $A$ and $B$ are independent, then \begin{align*}
        P(A|B) & = \frac{P(A \cap B)}{P(B)} \\
               & = \frac{P(A)P(B)}{P(B)}    \\
               & = P(A)
    \end{align*}
\end{lemma}

\begin{corollary}
    If $A$ and $B$ are independent, then $P(A|B) = P(A)$ and $P(B|A) = P(B)$
\end{corollary}

\subsection{Random Variables}

\begin{definition}
    A random variable is a function that takes outcomes from the sample space $\mathcal{S}$ to the real numbers $\mathbb{R}$. That is, a random variable is a function $X: \mathcal{S} \to \mathbb{R}$.
\end{definition}

We then use a probability mass function (pmf) in the discrete case or a
probability density function (pdf) in the continuous case:
\begin{align*}
     & \text{pmf:} &  & f_X(x) = P(X = x)                                   &  & \text{when } X \text{ is discrete}   \\
     & \text{pdf:} &  & \int_{a}^{b}f_X(x) \mathrm{d}x = P(a \leq X \leq b) &  & \text{when } X \text{ is continuous}
\end{align*}

The cumulative distribution function (cdf) gives the probability of observing a
value less than or equal to a given value $x$: \[F_X(x) = P(X \leq x)\]

When $X$ is a continuous random variable, the pdf is the derivative of the cdf: \[f_X(x) = F'_X(x)\]

\subsection{Expected Value and Variance}
For random variable $X$, the \vocab{expected value} is denoted by $E \,(X)$ and
is given by:
\[
    E(X) = \begin{cases}
        \sum_{x} x f_X(x)                            & \text{if } X \text{ is discrete}   \\
        \int_{-\infty}^{\infty} x f_X(x) \mathrm{d}x & \text{if } X \text{ is continuous}
    \end{cases}
\]
\\
The \vocab{variance} of a random variable $X$ is denoted by $\text{Var}(X)$ and is
given by:
\[
    \text{Var}(X) = E\left[(X - E(X))^{2}\right]
\]

\subsection{Covariance}

The \vocab{covariance} of two random variables $X$ and $Y$ is denoted by:
\[
    \text{Cov}(X, Y) = E\left[(X - E(X))(Y - E(Y))\right]
\]
\\
If two random variables $X$ and $Y$ are independent, then
\[
    P(X\in A, Y \in B) = P(X\in A)P(Y\in B)
\]
\\
So, we will be using these formulas to estimate the mean and the variance throughout the semester.
\section{January 8, 2025}
\subsection{Statistical Models}
In statistics, we often model data $X_1, X_2, \cdots, X_n$ as a random sample
from a population. We assume that the data are independent and identically
distributed (iid) random variables. The goal is to estimate the parameters of
the population distribution.

\begin{definition}
    A \vocab {parameter} of a distribution are values that describe a certain characteristic of the given distribution.
\end{definition}
Some examples of parameters include:
\begin{itemize}
    \item The mean height of \textbf{all} OSU incoming freshmen.
    \item The proportion of registered voters that voted for a particular candidate.
    \item The standard deviation of waiting times for \textbf{all} customers shopping at
          a store during a week.
\end{itemize}

\begin{fact}
    If $X_1, X_2, \ldots, X_n \overset{iid}{\approx} f_X(x)$ then \(\mu  = E(X_i)\) is a parameter, which is the mean of the distribution. The variance is also a parameter: \(\sigma^2 = E[(X - \mu)^2]\)
\end{fact}

\begin{example}
    Suppose we are examining the efficacy difference between a newly developed drug and an existing drug. We look at the differences, $\Delta i$, from a series of $n$ comparative samples. Note that these will all come from some distribution:
    \[ \Delta_1, \Delta_2, \ldots, \Delta_n, \overset{iid} f(x)\]

\end{example}
\begin{fact}
    Here the independed is a really important to look for we will look thorogh that through the semester.
\end{fact}
For a parametric model\[\{f_g(x)_{\theta \in \RR}\}\]
Which is indexed by a vector $\theta$ of parameters.

\begin{example}
    Suppose we wanted to estimate the height and weight of all incoming students at Ohio State. We could take a random sample of \(n\) of the incoming students and observe the height (\(H\)) and weight (\(W\)) of each student, giving the following sample data:
    \[
        (H_1, W_1), (H_2, W_2), \dots, (H_n, W_n)
    \]
\end{example}
We can then consider the following model:
\[
    {N(\mu, \Sigma)}
\]

\section{January 8, 2025}
We went over the \textbf{Recitation Logistics} and \textbf{Quiz 1}.

\section{January 10, 2025 (In-Person)}
We wanted to check how to get estimators. We will do the backwards this week for.

\subsection{Unbiased Estimator}
\begin{definition}
    An \vocab{estimator} \(\hat(\theta) \)
\end{definition}
\end{document}
